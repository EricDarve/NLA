<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
    <channel>
      <title>üìì CME 302</title>
      <link>https://ericdarve.github.io/NLA</link>
      <description>Last 10 notes on üìì CME 302</description>
      <generator>Quartz -- quartz.jzhao.xyz</generator>
      <item>
    <title>Bootcamp</title>
    <link>https://ericdarve.github.io/NLA/Bootcamp</link>
    <guid>https://ericdarve.github.io/NLA/Bootcamp</guid>
    <description>More during recitation + read the chapter in textbook for full background Vectors and matrices Introduce notations Subspace and linear independence Core concept in linear algebra Important when solving linear systems Dot product Shows up in many places.</description>
    <pubDate>Fri, 15 Sep 2023 20:16:00 GMT</pubDate>
  </item><item>
    <title>Cauchy-Schwarz</title>
    <link>https://ericdarve.github.io/NLA/Cauchy-Schwarz</link>
    <guid>https://ericdarve.github.io/NLA/Cauchy-Schwarz</guid>
    <description> Very useful in proofs to derive upper bounds, e.g., when analyzing the error or the convergence rate of a numerical algorithm ‚à£xTy‚à£‚â§‚à•x‚à•2‚Äã‚à•y‚à•2‚Äã Exercise: you can prove the triangle inequality using CS: ‚à•x+y‚à•2‚Äã‚â§‚à•x‚à•2‚Äã+‚à•y‚à•2‚Äã Dot product, Vector norms .</description>
    <pubDate>Fri, 15 Sep 2023 21:58:56 GMT</pubDate>
  </item><item>
    <title>Determinant</title>
    <link>https://ericdarve.github.io/NLA/Determinant</link>
    <guid>https://ericdarve.github.io/NLA/Determinant</guid>
    <description>The determinant is equal to the product of the eigenvalues: det(A)=i=1‚àèn‚ÄãŒªi‚Äã Geometric interpretation. For any square matrix A, we consider its columns ai‚Äã and the n-dimensional parallelepiped formed by the vectors ai‚Äã.</description>
    <pubDate>Fri, 15 Sep 2023 21:59:10 GMT</pubDate>
  </item><item>
    <title>Dot product</title>
    <link>https://ericdarve.github.io/NLA/Dot-product</link>
    <guid>https://ericdarve.github.io/NLA/Dot-product</guid>
    <description>Notation: xTy=x1‚Äãy1‚Äã+‚ãØ+xn‚Äãyn‚Äã Two vectors x and y are orthogonal if xTy=0. For a subspace S, we can define S‚ä• as the subspace of vectors y such that: S‚ä•={y‚à£‚àÄx‚ààS,xTy=0} We have: dim(S)+dim(S‚ä•)=n.</description>
    <pubDate>Fri, 15 Sep 2023 22:50:21 GMT</pubDate>
  </item><item>
    <title>Eigenvalues</title>
    <link>https://ericdarve.github.io/NLA/Eigenvalues</link>
    <guid>https://ericdarve.github.io/NLA/Eigenvalues</guid>
    <description>For any square matrix A there exists at least a scalar Œª‚ààC and x‚ààCn such that Ax=Œªx. They are called an eigenvalue and eigenvector. A matrix A is diagonalizable if there exists a basis x1‚Äã, ‚Ä¶, xn‚Äã of eigenvectors.</description>
    <pubDate>Fri, 15 Sep 2023 21:57:39 GMT</pubDate>
  </item><item>
    <title>First lecture</title>
    <link>https://ericdarve.github.io/NLA/First-lecture</link>
    <guid>https://ericdarve.github.io/NLA/First-lecture</guid>
    <description> My research NLA Parallel and HPC computing, GPU computing Machine learning, surrogate modeling, stochastic inversing, anomaly detection Cybersecurity Teaching team: Rajat Dwaraknath Ishani Karmarkar Chartsiri Jirachotkulthorn Style of class and what to expect This class will require some proof writing What is numerical linear algebra? Example: difference between the existence of eigenvalues and eigenvectors and how to compute them A class on numerical methods covers: Algorithms to solve a mathematical problem Computational cost Many algorithms are approximate; what is the error? how can it be controlled? how can it be estimated? Roundoff errors and stability: how do small errors in the data and during the calculation affect the final result? There will be some computer programming, but this is not the main focus We won‚Äôt discuss applications in detail The focus is on the algorithms, computational cost, and error analysis Content of class Solving linear systems: Ax=b.</description>
    <pubDate>Thu, 14 Sep 2023 18:28:59 GMT</pubDate>
  </item><item>
    <title>Hermitian and symmetric matrices</title>
    <link>https://ericdarve.github.io/NLA/Hermitian-and-symmetric-matrices</link>
    <guid>https://ericdarve.github.io/NLA/Hermitian-and-symmetric-matrices</guid>
    <description>The most important example of unitarily diagonalizable matrices. Complex Hermitian: A=AH. Then: A=QŒõQH and Œõ real. Real symmetric: A=AT. Then A=QŒõQT and Œõ and Q are real.</description>
    <pubDate>Fri, 15 Sep 2023 20:11:35 GMT</pubDate>
  </item><item>
    <title>Invertible matrix</title>
    <link>https://ericdarve.github.io/NLA/Invertible-matrix</link>
    <guid>https://ericdarve.github.io/NLA/Invertible-matrix</guid>
    <description>A matrix A is invertible if and only if there is a matrix B such that AB=BA=I We denote: B=A‚àí1. A matrix that is not invertible is said to be singular.</description>
    <pubDate>Fri, 15 Sep 2023 21:59:51 GMT</pubDate>
  </item><item>
    <title>Matrix-vector and matrix-matrix product</title>
    <link>https://ericdarve.github.io/NLA/Matrix-vector-and-matrix-matrix-product</link>
    <guid>https://ericdarve.github.io/NLA/Matrix-vector-and-matrix-matrix-product</guid>
    <description>Vectors and matrices, Dot product A matrix can be defined as a linear map from Rn‚ÜíRm. This is the operator view of a matrix: A:x‚ÜíAx. Algebraically, we write y=Ax with yi‚Äã=j=1‚àën‚Äãaij‚Äãxj‚Äã Using the operator interpretation, we can define the product of two matrices C=AB as the result of composing A with B: Cx=(AB)x=A(Bx) This works when B:Rn‚ÜíRp and A:Rp‚ÜíRm.</description>
    <pubDate>Fri, 15 Sep 2023 22:38:21 GMT</pubDate>
  </item><item>
    <title>Operator and matrix norms</title>
    <link>https://ericdarve.github.io/NLA/Operator-and-matrix-norms</link>
    <guid>https://ericdarve.github.io/NLA/Operator-and-matrix-norms</guid>
    <description>Measuring the size of a matrix ¬ß This is the main norm we will use. It can be derived from the Vector norms and extended to matrices ‚à•A‚à•p‚Äã=xÓÄ†=0sup‚Äã‚à•x‚à•p‚Äã‚à•Ax‚à•p‚Äã‚Äã=‚à•x‚à•p‚Äã=1max‚Äã‚à•Ax‚à•p‚Äã Example: A=(20‚Äã11‚Äã) 2-norm / 1-norm / ‚àû-norm: Other norms that are useful.</description>
    <pubDate>Fri, 15 Sep 2023 22:51:01 GMT</pubDate>
  </item>
    </channel>
  </rss>