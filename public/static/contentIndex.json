{"Solving-linear-systems":{"title":"Solving linear systems","links":[],"tags":[],"content":"How to solve Ax=b? One of the most important computational tasks in NLA.\nSolving triangular systems §\nSolve triangular system.\nDraw system and solution process.\nGeneral systems §\nAssume we have A=LU.\nShow how to solve in 2 steps.\nTriangular factorization §\nHow can we get L and U?\nProduct of matrices as a sum.\nA=BCaij​=k∑​bik​ckj​=bi1​c1j​+bi2​c2j​+…A=k∑​b,k​ck,​\nApplication to LU §\n[LU]ij​=li1​u1j​+li2​u2j​+⋯+lin​unj​LU=k∑​l,k​uk,​\n\nColumn and row notations:\n\nai,​: row i\na,j​: column j\n\n\n\nCompact notation:\nLU=k∑​l,k​uk,​\nExplain the process of computing the factors based on the sparsity pattern of the factors and the sum decomposition.\nEquations for column 1:\na,1​=l,1​u11​\nSolution with l11​=1 or u11​=1. Choose: l11​=1.\n\nFinal equations:\n\nu1,​=a1,​\nl,1​=a,1​/a11​\n\n\n\nFull algorithm\n\nLoop over k: 1 to n:\n\nuk,​=ak,​\nl,k​=a,k​/akk​\nA←A−l,k​∗uk,​\nThis is called the Schur complement.\n\n\n"},"Dot-product":{"title":"Dot product","links":["Vectors-and-matrices"],"tags":[],"content":"Notation:\nxTy=x1​y1​+⋯+xn​yn​\nTwo vectors x and y are orthogonal if xTy=0.\nFor a subspace S, we can define S⊥ as the subspace of vectors y such that:\nS⊥={y∣∀x∈S,xTy=0}\nWe have: dim(S)+dim(S⊥)=n.\nVectors and matrices"},"Singular-value-decomposition":{"title":"Singular value decomposition","links":["Operator-and-matrix-norms","The-four-fundamental-spaces","Eigenvalues","Orthogonal-matrix-and-projector"],"tags":[],"content":"One of the most important matrix decomposition in data science and machine learning.\nEigenvalues are great for understanding An. Does it grow? Does it shrink? How can it be easily modeled?\nBut it says nothing about the size of Ax. How is A transforming x and rescaling the vector?\nBecause A is a matrix, multiple scales are involved when applying A. These scales can be represented systematically using the singular value decomposition.\nA=UΣVT\n\nA: m×n.\nU: orthogonal, m×m. Left singular vectors\nV: orthogonal, n×n. Right singular vectors\nΣ: m×n, diagonal matrix with real positive entries = pure scaling = singular values.\n\nAvi​=σi​ui​\n\nConsider a ball B in Rn. A transforms this ball into an ellipsoid.\n\nAx=UΣVTx:\n\nVTx: point on the unit ball\nΣ(Vx): point on an ellipsoid; the axes are aligned with the coordinate axes.\nU(ΣVTx): rotate/reflect the ellipsoid.\n\n\n\nThe lengths of the axes of this ellipsoid are the singular values of A.\n\nAs we can expect, the size of a matrix (Operator and matrix norms) can be related to its singular values:\n∥A∥2​=σ1​(A)∥A∥F​=i=1∑p​σi2​​\nThe four fundamental spaces. Assume A is n×m. r: number of non-zero singular values = rank of the matrix. Then:\nN(A)R(A)N(AT)R(AT)​={vr+1​,…,vm​}={u1​,…,ur​}={ur+1​,…,un​}=R(A)⊥={v1​,…,vr​}=N(A)⊥​\nWe also recover the rank-nullity theorem.\nThe four fundamental spaces, Eigenvalues, Operator and matrix norms, Orthogonal matrix and projector"},"Bootcamp":{"title":"Bootcamp","links":["Vectors-and-matrices","Subspace-and-linear-independence","Dot-product","Vector-norms","Pythagorean-theorem","Cauchy-Schwarz","Matrix-vector-and-matrix-matrix-product","Invertible-matrix","Sherman-Morrison-Woodbury-formula","Operator-and-matrix-norms","The-four-fundamental-spaces","Orthogonal-matrix-and-projector","Eigenvalues","Determinant","Trace","Unitarily-diagonalizable-matrices","Hermitian-and-symmetric-matrices","Schur-decomposition","Singular-value-decomposition"],"tags":[],"content":"More during recitation + read the chapter in textbook for full background\n\nVectors and matrices\n\nIntroduce notations\n\n\nSubspace and linear independence\n\nCore concept in linear algebra\nImportant when solving linear systems\n\n\nDot product\n\nShows up in many places.\nExample: vector norms, matrix-vector and matrix-matrix products.\nUsed to define orthogonality\n\n\nVector norms\n\nHow to measure things\nKey to calculating errors in numerical methods\n\n\nPythagorean theorem\n\nHow to simply calculate the length of a vector given its decomposition into orthogonal subspaces\nThis is key to computing the norm of a vector in certain situations.\n\n\nCauchy-Schwarz\n\nKey in proofs to derive upper bounds on error\nConsidered one of the most important and widely used inequalities in mathematics\n\n\nMatrix-vector and matrix-matrix product\n\nCan be either viewed algebraically or interpreted as an operator\nMatrix-vector: applying a linear operator to transform a vector\nMatrix-matrix: operator composition\n\n\nInvertible matrix\n\nA requirement to solve a linear system and obtain a unique solution\n\n\nSherman-Morrison-Woodbury formula\n\nHow to solve a linear system when we make a small perturbation\n\n\nOperator and matrix norms\n\nMeasuring the size of operators\nKey when deriving error bounds and for proof.\n\n\nThe four fundamental spaces\n\nUnderstanding the structure of linear operators\nHow they transform the input vector and map subspaces\n\n\nOrthogonal matrix and projector\n\nOrthogonal matrices will be key because they act as isometries\nUseful for building algorithms with low error\nKey in many matrix decompositions or factorizations\n\n\nEigenvalues\n\nKey to analyzing powers of a matrix: Ak.\nThis is important for time evolution and repeated applications of an operator\nLong-term evolution of a dynamical system\nNot useful to understand what happens when applying the operator once\n\n\nDeterminant\n\nHow a matrix changes the volume of a subspace\n\n\nTrace\n\nConnect matrix with vector field\nTrace = divergence of vector field = flux through a unit square\n\n\nUnitarily diagonalizable matrices\n\nThe simplest and most accurate case\nDiagonalizable + orthogonal matrices!\n\n\nHermitian and symmetric matrices\n\nAn important special case of Unitarily diagonalizable matrices\n\n\nSchur decomposition\n\nThis will be key to computing eigenvalues\nThe fact that it uses orthogonal matrices will be important to ensure the accuracy of the algorithm\nExists for all square matrices\n\n\nSingular value decomposition\n\nKey to understanding how a matrix scales and transforms space\nA linear operator always transforms the unit ball to an n-ellipsoid.\nConnection with The four fundamental spaces.\nConnection with Eigenvalues and determinant of a matrix.\nKey when solving least-squares problems: minx​∥Ax−b∥2​.\nRelation to Operator and matrix norms\n\n\n"},"Invertible-matrix":{"title":"Invertible matrix","links":["Matrix-vector-and-matrix-matrix-product"],"tags":[],"content":"A matrix A is invertible if and only if there is a matrix B such that\nAB=BA=I\nWe denote: B=A−1. A matrix that is not invertible is said to be singular.\nLinear systems have a unique solution if the matrix is invertible.\nMatrix-vector and matrix-matrix product"},"Unitarily-diagonalizable-matrices":{"title":"Unitarily diagonalizable matrices","links":["Eigenvalues","Orthogonal-matrix-and-projector"],"tags":[],"content":"A matrix A∈Cn×n is unitarily diagonalizable if there is a unitary matrix Q and a diagonal matrix Λ so that\nA=QΛQH\nDefinition: A is normal if and only if AHA=AAH.\nProperty: all normal matrices are unitarily diagonalizable.\nEigenvalues, Orthogonal matrix and projector"},"Vector-norms":{"title":"Vector norms","links":["Dot-product","Vectors-and-matrices"],"tags":[],"content":"\nKey tool for proofs and formal derivations\nEssential to describe and understand properties of objects in LA\nUsed to convert a vector to a single scalar number, its “size.”\nMost common norm is the 2-norm:\n\n∥x∥2​=(i=1∑n​(xi​)2)1/2=xTx​\nDot product and the 2-norm are connected through:\nxTy=∥x∥2​∥y∥2​cosθ\nSo the dot product can be used to measure the angle between two vectors.\nIf two vectors are orthogonal, their dot product is equal to 0.\nWe can also use the dot product to project a vector unto an other:\n\nProjection of y unto x: xTy/∥x∥2​.\n\nIn data science and linear algebra, it’s common to use different norms. They differ by the weight they assign to the components of a vector.\n\n1-norm: all scales contribute equally\n\n∥x∥1​=i=1∑n​∣xi​∣\n\n2-norm: emphasizes the larger entries\nLargest entry:\n\n∥x∥∞​=1≤i≤nmax​∣xi​∣\n\nIntermediate; as p→∞, norm becomes closer to max-norm\n\n∥x∥p​=(i=1∑n​∣xi​∣p)1/p\nBalls in different norms\n\nVectors and matrices"},"Subspace-and-linear-independence":{"title":"Subspace and linear independence","links":["Vectors-and-matrices"],"tags":[],"content":"Vector subspace §\nVector space: a set of elements that can be added together and multiplied by scalars (for this class, in R or C). Vectors and matrices\nExample: Rn.\nA subspace of Rn is a vector space that is a subset of Rn.\nExample: take k vectors x1​, …, xk​, you can check that all vectors of the form\na1​x1​+⋯+ak​xk​\nform a subspace. We will denote this subspace as S=span{x1​,…,xk​}.\nLinear independence §\nWe say that x1​, …, xk​ are linearly independent if and only if any vector x∈S has a unique decomposition as\nx=a1​x1​+⋯+ak​xk​\nIn particular, if\n0=a1​x1​+⋯+ak​xk​\nthen a1​=⋯=ak​=0.\nIf the vectors are linearly independent, k is the dimension of S.\nLinear independence will be important when solving linear systems. It will guarantee that the solution is unique.\nDirect sum §\nIf U and V are subspaces, then U+V is a subspace. We say that W=U⊕V is the direct sum of U and V if U∩V={0}. The direct sum means that if a vector is decomposed into its U and V components, this decomposition is unique.\nExample: verify that if x1​, …, xk​ are linearly independent, then\nS=span{x1​}⊕⋯⊕span{xk​}"},"Matrix-vector-and-matrix-matrix-product":{"title":"Matrix-vector and matrix-matrix product","links":["Vectors-and-matrices","Dot-product"],"tags":[],"content":"Vectors and matrices, Dot product\nA matrix can be defined as a linear map from Rn→Rm. This is the operator view of a matrix: A:x→Ax.\nAlgebraically, we write\ny=Ax\nwith\nyi​=j=1∑n​aij​xj​\nUsing the operator interpretation, we can define the product of two matrices C=AB as the result of composing A with B:\nCx=(AB)x=A(Bx)\nThis works when B:Rn→Rp and A:Rp→Rm.\nAlgebraically, we have\ncij​=k=1∑p​aik​bkj​\nFor the product to be defined, the number of columns of A, p, must be equal to the number of rows of B."},"Hermitian-and-symmetric-matrices":{"title":"Hermitian and symmetric matrices","links":["Unitarily-diagonalizable-matrices","Orthogonal-matrix-and-projector"],"tags":[],"content":"The most important example of unitarily diagonalizable matrices.\nComplex Hermitian: A=AH. Then: A=QΛQH and Λ real.\nReal symmetric: A=AT. Then A=QΛQT and Λ and Q are real.\nUnitarily diagonalizable matrices, Orthogonal matrix and projector"},"Operator-and-matrix-norms":{"title":"Operator and matrix norms","links":["Vector-norms","Vectors-and-matrices","Matrix-vector-and-matrix-matrix-product"],"tags":[],"content":"Measuring the size of a matrix §\nThis is the main norm we will use. It can be derived from the Vector norms and extended to matrices\n∥A∥p​=x=0sup​∥x∥p​∥Ax∥p​​=∥x∥p​=1max​∥Ax∥p​\nExample: A=(20​11​)\n2-norm / 1-norm / ∞-norm:\n\nOther norms that are useful.\nFrobenius norm: it is very convenient to analyze matrices that can be decomposed into blocks\n∥A∥F​=(ij∑​aij2​)1/2\nUseful inequalities for proofs §\n∥Ax∥p​≤∥A∥p​∥x∥p​\nThis is a consequence of the definition of the norm. Exercise: prove this result using the definition.\nFor Frobenius and p-norms:\n∥AB∥≤∥A∥∥B∥\nThis is called a sub-multiplicative norm.\nVectors and matrices, Vector norms, Matrix-vector and matrix-matrix product"}}