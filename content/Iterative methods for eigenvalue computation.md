- [[Motivation of iterative methods for eigenvalue computation]]
	- We briefly highlight why a different type of method is required for sparse matrices.
- [[Key idea of iterative methods for eigenvalue computation]]
	- The key idea goes back to the previous section, where we learned how to transform a matrix to the [[Upper Hessenberg form for the QR iteration|upper Hessenberg]] form.
	- We can calculate the columns of $H$ one by one by following a process similar to [[Gram-Schmidt]].
- [[Brief introduction to Conjugate Gradients]]
	- We use the approach from the [[Key idea of iterative methods for eigenvalue computation|previous section]] to find an approximate solution for sparse linear systems $Ax = b$.
- [[The Arnoldi process]]
	- We use the algorithm from the [[Key idea of iterative methods for eigenvalue computation|section above]] to approximate the eigenvalues of $A$.
	- Only simple vector operations and sparse matrix-vector products are required. So this approach is computationally very efficient for small $k$.
- [[Algorithm for the Arnoldi process]]
	- We provide the details of the algorithm using pseudo-code.
	- See movies of convergence and `Arnoldi convergence.key`
	- Understanding convergence is not an easy task and is a complex topic.

