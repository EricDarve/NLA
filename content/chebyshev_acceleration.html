
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Chebyshev Iteration Method &#8212; CME 302 Numerical Linear Algebra</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chebyshev_acceleration';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Krylov Subspace Methods" href="krylov_iterative_methods.html" />
    <link rel="prev" title="Successive Over-Relaxation (SOR) Method" href="sor_method.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">CME 302 Numerical Linear Algebra</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Class Notes 2025
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="bootcamp.html">Linear Algebra Bootcamp</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="vector_space.html">Vector spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="dot_product_and_norms.html">Dot Product and Vector Norms</a></li>
<li class="toctree-l2"><a class="reference internal" href="linear_transformations.html">Linear Transformations and Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="matrix_matrix_multiplication.html">Matrix-Matrix Multiplications</a></li>
<li class="toctree-l2"><a class="reference internal" href="operator_norms.html">Operator and Matrix Norms</a></li>
<li class="toctree-l2"><a class="reference internal" href="sherman_morrison_woodbury.html">The Sherman-Morrison-Woodbury Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="determinants.html">The Determinant</a></li>
<li class="toctree-l2"><a class="reference internal" href="trace.html">The Trace of a Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="orthogonal_matrices.html">Orthogonal Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="projections.html">Projections</a></li>
<li class="toctree-l2"><a class="reference internal" href="block_matrices.html">Block Matrix Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="four_fundamental_subspaces.html">The Four Fundamental Subspaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">Eigendecomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="normal_matrices.html">Normal matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="applications_of_eigenvalues.html">Applications of Eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="singular_value_decomposition.html">Singular Value Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigen_vs_singular_values.html">Eigenvalues and Singular Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="summary_of_matrix_decompositions.html">Summary of Matrix Decompositions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="solving_linear_systems.html">Solving Linear Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="lu_decomposition.html">The LU Decomposition Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="existence_lu.html">Existence and Uniqueness of LU Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="floating_point.html">Floating-Point Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="lu_pivoting.html">LU Factorization with Row Pivoting</a></li>
<li class="toctree-l2"><a class="reference internal" href="cholesky.html">Cholesky Factorization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="least_squares.html">Least Squares Problems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="householder_reflections.html">Householder Reflections</a></li>
<li class="toctree-l2"><a class="reference internal" href="givens_rotations.html">Givens Rotations</a></li>
<li class="toctree-l2"><a class="reference internal" href="modified_gram_schmidt.html">Modified Gram-Schmidt</a></li>
<li class="toctree-l2"><a class="reference internal" href="qr_summary.html">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="existence_and_uniqueness_qr.html">Existence and Uniqueness of QR Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="backward_stability.html">Backward Stability of Householder and Givens QR</a></li>
<li class="toctree-l2"><a class="reference internal" href="qr_and_determinant.html">The QR Factorization and the Determinant</a></li>
<li class="toctree-l2"><a class="reference internal" href="lu_vs_qr.html">LU vs. QR Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="normal_equations.html">The Method of Normal Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="LS_using_QR.html">Solving Least-Squares using QR Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="LS_using_SVD.html">SVD for Rank-Deficient Least-Squares</a></li>
<li class="toctree-l2"><a class="reference internal" href="LS_summary.html">Summary of LS Solution Methods</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="eigenvalues.html">Eigenvalue Computation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="power_method.html">The Power Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="deflation.html">The Method of Deflation</a></li>
<li class="toctree-l2"><a class="reference internal" href="orthogonal_iteration.html">The Orthogonal Iteration Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="orthogonal_and_power_iteration.html">Power and Orthogonal Iteration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="orthogonal_iteration_and_eigenvalues.html">Orthogonal Iteration and Eigenvalues</a></li>
<li class="toctree-l2"><a class="reference internal" href="computing_the_eigenvectors.html">Computing Eigenvectors from the Schur Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="qr_iteration.html">QR Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="upper_hessenberg.html">Reduction to Hessenberg Form</a></li>
<li class="toctree-l2"><a class="reference internal" href="qr_iteration_with_hessenberg.html">QR iteration for upper Hessenberg matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="qr_iteration_with_shifts.html">The Shifted QR Iteration Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="symmetric_vs_nonsymmetric_qr.html">Symmetric vs Unsymmetric QR Iteration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="eigenvalues_iterative.html">Iterative Methods for Eigenvalue Computation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="krylov_subspaces.html">Projection onto Krylov Subspaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="arnoldi_iteration.html">The Arnoldi Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="arnoldi_convergence.html">Convergence of the Arnoldi Process</a></li>

<li class="toctree-l2"><a class="reference internal" href="lanczos_iteration.html">The Lanczos Algorithm: Arnoldi for Symmetric Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="lanczos_convergence.html">Convergence of the Lanczos Process</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="classical_iterative_methods.html">Classical Iterative Methods</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="splitting_methods_definition.html">Splitting Methods and Convergence Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="jacobi_method.html">The Jacobi Iteration Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="gauss_seidel_method.html">The Gauss-Seidel Iteration Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="sor_method.html">Successive Over-Relaxation (SOR) Method</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">The Chebyshev Iteration Method</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="krylov_iterative_methods.html">Krylov Subspace Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="conjugate_gradient.html">The Conjugate Gradient (CG) Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="gmres.html">The Generalized Minimal Residual Method (GMRES)</a></li>
<li class="toctree-l2"><a class="reference internal" href="cg_vs_gmres.html">A Comparative Summary: CG vs. GMRES</a></li>
<li class="toctree-l2"><a class="reference internal" href="minres.html">MINRES</a></li>
<li class="toctree-l2"><a class="reference internal" href="preconditioning.html">Preconditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="pcg.html">Preconditioning the Conjugate Gradient (PCG) Algorithm</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="LICENSE.html">License for this book</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/content/chebyshev_acceleration.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Chebyshev Iteration Method</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-stationary-to-nonstationary-iteration">From Stationary to Nonstationary Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-and-matrix-polynomials">Error Propagation and Matrix Polynomials</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-polynomial-optimization-problem">The Polynomial Optimization Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-representation-and-the-minimax-problem">Error Representation and the Minimax Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-optimal-polynomial-p-k-lambda">Deriving the Optimal Polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-exact-bound">Calculating the Exact Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asymptotic-rate">Asymptotic Rate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-the-1st-order-chebyshev-iterative-method">Limitations of the 1st Order Chebyshev Iterative Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-build-the-2nd-order-chebyshev-iterative-method">How to Build the 2nd Order Chebyshev Iterative Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-the-2nd-order-chebyshev-iterative-method">Advantages of the 2nd Order Chebyshev Iterative Method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements-and-sensitivity-to-eigenvalue-bounds">Requirements and Sensitivity to Eigenvalue Bounds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-cons-and-applications">Pros, Cons, and Applications</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-chebyshev-iteration-method">
<h1>The Chebyshev Iteration Method<a class="headerlink" href="#the-chebyshev-iteration-method" title="Link to this heading">#</a></h1>
<p>The Chebyshev iteration method, also known as a Chebyshev semi-iterative method, is a powerful technique for accelerating the convergence of iterative solvers for the linear system <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span>. It is a <strong>nonstationary iterative method</strong>, meaning it improves upon basic stationary schemes (like the Richardson iteration) by using acceleration parameters <span class="math notranslate nohighlight">\(\tau_k\)</span> that change at each step <span class="math notranslate nohighlight">\(k\)</span>. This strategy, which leverages the properties of Chebyshev polynomials, can result in significantly faster convergence.</p>
<section id="from-stationary-to-nonstationary-iteration">
<h2>From Stationary to Nonstationary Iteration<a class="headerlink" href="#from-stationary-to-nonstationary-iteration" title="Link to this heading">#</a></h2>
<p>Classical iterative methods can be expressed using the preconditioned “residual update” form. Let’s start with the familiar splitting-style approach:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
A &amp;= M - N \\
\mathbf{x}^{(k+1)} &amp;= M^{-1} (\mathbf{b} + N \mathbf{x}^{(k)}) \\
\mathbf{x}^{(k+1)} &amp;= M^{-1} (\mathbf{b} + (M - A) \mathbf{x}^{(k)})
\end{aligned}
\end{split}\]</div>
<p>This implies that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + M^{-1} (\mathbf{b} - A \mathbf{x}^{(k)})
\]</div>
<p>This is the residual iteration form. If we define <span class="math notranslate nohighlight">\(\tilde{A} = M^{-1}A\)</span> and <span class="math notranslate nohighlight">\(\tilde{\mathbf{b}} = M^{-1}\mathbf{b}\)</span>, the iteration becomes:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + (\tilde{\mathbf{b}} - \tilde{A}\mathbf{x}^{(k)})\]</div>
<p>This is the basic <strong>Richardson iteration</strong>. A stationary method would apply a single, fixed acceleration parameter <span class="math notranslate nohighlight">\(\tau\)</span>. The key idea of a nonstationary method is to allow this parameter to vary at each step.</p>
<p>For the remainder of this analysis, we will assume such preconditioning <span class="math notranslate nohighlight">\(M^{-1}A\)</span> has already been applied and (for notational simplicity) analyze the iteration for the system <span class="math notranslate nohighlight">\(A\mathbf{x} = \mathbf{b}\)</span>. The general nonstationary form is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \tau_k (\mathbf{b} - A\mathbf{x}^{(k)}), \quad k=0, 1, \dots\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\{\tau_k\}\)</span> is the sequence of step-varying parameters.</p>
</section>
<section id="error-propagation-and-matrix-polynomials">
<h2>Error Propagation and Matrix Polynomials<a class="headerlink" href="#error-propagation-and-matrix-polynomials" title="Link to this heading">#</a></h2>
<p>We can analyze the method’s convergence by tracking the error <span class="math notranslate nohighlight">\(\mathbf{e}^{(k)} = \mathbf{x} - \mathbf{x}^{(k)}\)</span>. We can derive the recurrence for the error as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{e}^{(k+1)} &amp;= \mathbf{x} - \mathbf{x}^{(k+1)} \\
&amp;= \mathbf{x} - \left[ \mathbf{x}^{(k)} + \tau_k (\mathbf{b} - A\mathbf{x}^{(k)}) \right] \\
&amp;= (\mathbf{x} - \mathbf{x}^{(k)}) - \tau_k (A\mathbf{x} - A\mathbf{x}^{(k)}) \quad (\text{since } \mathbf{b} = A\mathbf{x}) \\
&amp;= \mathbf{e}^{(k)} - \tau_k A \mathbf{e}^{(k)} \\
\mathbf{e}^{(k+1)} &amp;= (I - \tau_k A) \mathbf{e}^{(k)}
\end{aligned}
\end{split}\]</div>
<p>By unrolling this recurrence, we can express the error after <span class="math notranslate nohighlight">\(k\)</span> steps as the result of a matrix polynomial applied to the initial error <span class="math notranslate nohighlight">\(\mathbf{e}^{(0)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{e}^{(k)} = (I - \tau_{k-1} A) \cdots (I - \tau_1 A) (I - \tau_0 A) \mathbf{e}^{(0)}\]</div>
<p>We define this product as the <strong>matrix polynomial</strong> <span class="math notranslate nohighlight">\(P_k(A)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{e}^{(k)} = P_k(A) \mathbf{e}^{(0)} \quad \text{where} \quad P_k(A) = \prod_{l=0}^{k-1} (I - \tau_l A)\]</div>
</section>
<section id="the-polynomial-optimization-problem">
<h2>The Polynomial Optimization Problem<a class="headerlink" href="#the-polynomial-optimization-problem" title="Link to this heading">#</a></h2>
<p>The effectiveness of the iteration after <span class="math notranslate nohighlight">\(k\)</span> steps is determined entirely by the properties of this <span class="math notranslate nohighlight">\(k\)</span>-th degree polynomial. This construction imposes two fundamental constraints on the corresponding scalar polynomial <span class="math notranslate nohighlight">\(P_k(\lambda) = \prod_{l=0}^{k-1} (1 - \tau_l \lambda)\)</span>:</p>
<ol class="arabic simple">
<li><p><strong>Degree:</strong> <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span> is a polynomial of degree <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p><strong>Normalization:</strong> The polynomial must satisfy <span class="math notranslate nohighlight">\(P_k(0) = 1\)</span>. This is self-evident from the product form (setting <span class="math notranslate nohighlight">\(\lambda=0\)</span>) and is a necessary constraint for any method derived from this iterative form.</p></li>
</ol>
<p>From this factorization, we also gain a key insight: the iteration parameters <span class="math notranslate nohighlight">\(\{\tau_l\}\)</span> are the <strong>inverses of the roots</strong> of the scalar polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span>, since <span class="math notranslate nohighlight">\(P_k(1/\tau_l) = 0\)</span>.</p>
<p>The central goal of the Chebyshev method is to choose the sequence of parameters <span class="math notranslate nohighlight">\(\{\tau_l\}\)</span> to minimize the norm of the error <span class="math notranslate nohighlight">\(\mathbf{e}^{(k)}\)</span> in the fastest possible way. Since <span class="math notranslate nohighlight">\(\mathbf{e}^{(k)} = P_k(A) \mathbf{e}^{(0)}\)</span>, this is equivalent to finding the polynomial <span class="math notranslate nohighlight">\(P_k(A)\)</span> that is “as small as possible” in some sense.</p>
<p>If <span class="math notranslate nohighlight">\(A\)</span> is diagonalizable, its eigenvectors <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span> corresponding to eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span> are mapped as:</p>
<div class="math notranslate nohighlight">
\[P_k(A)\mathbf{v}_i = P_k(\lambda_i)\mathbf{v}_i\]</div>
<p>This shows that the error components in the direction of <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span> are damped by a factor of <span class="math notranslate nohighlight">\(P_k(\lambda_i)\)</span>. The optimization problem is thus reduced to finding a polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span> of degree <span class="math notranslate nohighlight">\(k\)</span> that satisfies:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P_k(0) = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P_k(\lambda)\)</span> is as small as possible across the spectrum of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\sigma(A)\)</span>.</p></li>
</ol>
<p>This strategy—finding the optimal polynomial that is small on the spectrum while constrained to be 1 at the origin—is the foundation of the Chebyshev iteration method.</p>
<p>The convergence analysis of the Chebyshev iteration method is rooted in optimization theory, specifically solving a <strong>minimax polynomial approximation problem</strong>. The goal is to construct a polynomial that is maximally suppressed over the range of eigenvalues while satisfying a critical normalization condition.</p>
</section>
<section id="error-representation-and-the-minimax-problem">
<h2>Error Representation and the Minimax Problem<a class="headerlink" href="#error-representation-and-the-minimax-problem" title="Link to this heading">#</a></h2>
<p>Since the eigenvalues of <span class="math notranslate nohighlight">\(P_k(A)\)</span> are <span class="math notranslate nohighlight">\(P_k(\lambda_i)\)</span> for eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span> of <span class="math notranslate nohighlight">\(A\)</span>, the convergence rate (determined by the spectral radius <span class="math notranslate nohighlight">\(\rho(P_k(A))\)</span>) is minimized by solving the following <strong>minimax problem</strong>:</p>
<div class="math notranslate nohighlight">
\[\min_{P_k \in \Pi_k, P_k(0)=1} \max_{\lambda \in [a, b]} |P_k(\lambda)|\]</div>
<p>where <span class="math notranslate nohighlight">\(\Pi_k\)</span> is the set of polynomials of degree at most <span class="math notranslate nohighlight">\(k\)</span>, and <span class="math notranslate nohighlight">\([a, b]\)</span> is the interval containing the eigenvalues of <span class="math notranslate nohighlight">\(A\)</span>, with <span class="math notranslate nohighlight">\(0 &lt; a \le b\)</span>.</p>
<section id="deriving-the-optimal-polynomial-p-k-lambda">
<h3>Deriving the Optimal Polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span><a class="headerlink" href="#deriving-the-optimal-polynomial-p-k-lambda" title="Link to this heading">#</a></h3>
<p>The function that solves this specific minimax problem is a suitably scaled and shifted Chebyshev polynomial of the first kind, <span class="math notranslate nohighlight">\(T_k(z)\)</span>.</p>
<p><strong>Transformation to the Standard Interval</strong></p>
<p>The standard Chebyshev polynomial <span class="math notranslate nohighlight">\(T_k(z)\)</span> minimizes its maximum magnitude over the interval <span class="math notranslate nohighlight">\(z \in [-1, 1]\)</span>. To map the eigenvalue interval <span class="math notranslate nohighlight">\(\lambda \in [a, b]\)</span> to this standard interval, a linear transformation is required:</p>
<div class="math notranslate nohighlight">
\[z(\lambda) = \frac{a + b - 2\lambda}{b - a}\]</div>
<p>This linear transformation maps <span class="math notranslate nohighlight">\(\lambda \in [a, b]\)</span> to <span class="math notranslate nohighlight">\(z \in [-1, 1]\)</span>, reversing the order of the interval.</p>
<p>This transformation ensures that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda = a \implies z = 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda = b \implies z = -1\)</span></p></li>
</ul>
<p><strong>Normalization</strong></p>
<p>The optimal polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span> is then constructed using <span class="math notranslate nohighlight">\(T_k(z(\lambda))\)</span> but requires normalization to enforce the constraint <span class="math notranslate nohighlight">\(P_k(0)=1\)</span>. This is achieved by dividing by the polynomial’s value at the origin:</p>
<div class="math notranslate nohighlight">
\[P_k(\lambda) = \frac{T_k(z(\lambda))}{T_k(z(0))}\]</div>
</section>
<section id="calculating-the-exact-bound">
<h3>Calculating the Exact Bound<a class="headerlink" href="#calculating-the-exact-bound" title="Link to this heading">#</a></h3>
<p>The convergence bound arises from evaluating the magnitude of the optimal polynomial at its maximal points over the eigenvalue interval <span class="math notranslate nohighlight">\([a, b]\)</span>.</p>
<p><strong>Evaluating the Maximum Modulus of the Numerator</strong></p>
<p>For <span class="math notranslate nohighlight">\(\lambda \in [a, b]\)</span>, the transformed variable <span class="math notranslate nohighlight">\(z(\lambda)\)</span> lies in <span class="math notranslate nohighlight">\([-1, 1]\)</span>. In this range, the Chebyshev polynomial’s defining property ensures that its maximum modulus is unity:</p>
<div class="math notranslate nohighlight">
\[\max_{\lambda \in [a, b]} |T_k(z(\lambda))| = \max_{z \in [-1, 1]} |T_k(z)| = 1\]</div>
<p><strong>Evaluating the Normalization Constant</strong></p>
<div class="math notranslate nohighlight">
\[z(0) = \frac{b+a - 2(0)}{b-a} = \frac{b+a}{b-a}\,.\]</div>
<p>Since <span class="math notranslate nohighlight">\(a&gt;0\)</span>, <span class="math notranslate nohighlight">\(b&gt;a\)</span>, the argument <span class="math notranslate nohighlight">\(z(0) = \frac{b+a}{b-a}\)</span> is greater than <span class="math notranslate nohighlight">\(1\)</span>. Substituting these results into the definition of the optimal polynomial gives the final exact bound for the maximum error reduction:</p>
<div class="math notranslate nohighlight">
\[\min_{P_k \in \Pi_k, P_k(0)=1} \max_{a \le \lambda \le b} |P_k(\lambda)| = \frac{\max_{\lambda \in [a, b]} |T_k(z(\lambda))|}{|T_k(z(0))|} = \frac{1}{\Big|T_k(\frac{b+a}{b-a})\Big|}\]</div>
</section>
<section id="asymptotic-rate">
<h3>Asymptotic Rate<a class="headerlink" href="#asymptotic-rate" title="Link to this heading">#</a></h3>
<p><strong>Error Norm Application</strong></p>
<p>Recall that:</p>
<div class="math notranslate nohighlight">
\[\mathbf{e}^{(k)} = P_k(A) \mathbf{e}^{(0)} = X P_k(\Lambda) X^{-1} \mathbf{e}^{(0)}\]</div>
<p>Assume that <span class="math notranslate nohighlight">\(A\)</span> is symmetric positive definite (SPD), so it has an orthonormal eigenvector basis. Then:</p>
<div class="math notranslate nohighlight">
\[\|\mathbf{e}^{(k)}\|_2 \le \|P_k(A)\|_2 \|\mathbf{e}^{(0)}\|_2 = \max_{a \le \lambda \le b} |P_k(\lambda)| \; \|\mathbf{e}^{(0)}\|_2\]</div>
<p><strong>Asymptotic Approximation and <span class="math notranslate nohighlight">\(\sigma\)</span></strong></p>
<p>For large <span class="math notranslate nohighlight">\(k\)</span>, the Chebyshev polynomial <span class="math notranslate nohighlight">\(T_k(z)\)</span> for <span class="math notranslate nohighlight">\(z &gt; 1\)</span> is approximated by its exponential form (hyperbolic cosine):</p>
<div class="math notranslate nohighlight">
\[T_k(z) \approx \frac{1}{2} (z + \sqrt{z^2 - 1})^k\]</div>
<p>When <span class="math notranslate nohighlight">\(z_0 = \frac{b+a}{b-a}\)</span>, the term <span class="math notranslate nohighlight">\(z_0 + \sqrt{z_0^2 - 1}\)</span> simplifies, defining the asymptotic reduction rate <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[\|\mathbf{e}^{(k)}\|_2 \lessapprox 2 \cdot \left(\frac{1 - \sqrt{\frac{a}{b}}}{1 + \sqrt{\frac{a}{b}}}\right)^k \|\mathbf{e}^{(0)}\|_2 = 2 \sigma^k \|\mathbf{e}^{(0)}\|_2\]</div>
<p>where the reduction rate is defined as:</p>
<div class="math notranslate nohighlight">
\[\sigma = \frac{1 - \sqrt{a/b}}{1 + \sqrt{a/b}}\]</div>
<p>This reduction rate <span class="math notranslate nohighlight">\(\sigma\)</span> shows that the number of iterations required is proportional to <span class="math notranslate nohighlight">\(\ln(2/\epsilon) / \ln(1/\sigma)\)</span>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the desired error tolerance. Since <span class="math notranslate nohighlight">\(\sigma\)</span> is approximately <span class="math notranslate nohighlight">\(1 - 2/\sqrt{\kappa}\)</span> when the condition number <span class="math notranslate nohighlight">\(\kappa = b/a\)</span> is large, the convergence rate is proportional to <span class="math notranslate nohighlight">\(\sqrt{\kappa}\)</span>, which explains the superior efficiency of Chebyshev iteration compared to linear convergence methods proportional to <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>The convergence analysis hinges on finding the polynomial roots (the iteration parameters <span class="math notranslate nohighlight">\(\tau_l\)</span>) that distribute the suppression across the eigenvalue interval optimally, minimizing the largest residual component left after <span class="math notranslate nohighlight">\(k\)</span> steps. The Chebyshev iteration achieves this by having the roots of <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span> correspond to the specific <strong>zeros of <span class="math notranslate nohighlight">\(T_k(z)\)</span></strong> transformed back to the <span class="math notranslate nohighlight">\(\lambda\)</span> axis.</p>
</section>
</section>
<section id="limitations-of-the-1st-order-chebyshev-iterative-method">
<h2>Limitations of the 1st Order Chebyshev Iterative Method<a class="headerlink" href="#limitations-of-the-1st-order-chebyshev-iterative-method" title="Link to this heading">#</a></h2>
<p>The first-order Chebyshev iterative method is defined by the one-step recurrence relation:</p>
<div class="math notranslate nohighlight">
\[x^{(k+1)} = x^{(k)} - \tau_k(Ax^{(k)} - b).\]</div>
<p>The limitations associated with this method are:</p>
<ul class="simple">
<li><p><strong>A Priori Selection of Steps:</strong> The method requires the user to choose the total number of iteration steps <span class="math notranslate nohighlight">\(k\)</span> beforehand. This must be done using estimation formulas based on the desired accuracy and the eigenvalue bounds.</p></li>
<li><p><strong>Numerical Instability:</strong> The major disadvantage is that the method can be <strong>numerically unstable</strong>. This instability arises because for certain indices <span class="math notranslate nohighlight">\(k\)</span>, the iteration matrices <span class="math notranslate nohighlight">\(I - \tau_k A\)</span> can have a spectral radius much larger than <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p><strong>Parameter Ordering Requirement:</strong> To mitigate the instability, the iteration parameters <span class="math notranslate nohighlight">\(\tau_k\)</span> must be selected and used in a specific order (so that large values of <span class="math notranslate nohighlight">\(\tau_k\)</span> are balanced by small values).</p></li>
</ul>
<section id="how-to-build-the-2nd-order-chebyshev-iterative-method">
<h3>How to Build the 2nd Order Chebyshev Iterative Method<a class="headerlink" href="#how-to-build-the-2nd-order-chebyshev-iterative-method" title="Link to this heading">#</a></h3>
<p>The 2nd order Chebyshev iterative method is developed to eliminate the disadvantages of the 1st order method—namely, the requirement to choose the total number of iteration steps <span class="math notranslate nohighlight">\(k\)</span> beforehand and the potential numerical instability. The two-step version is stable across iterations when the eigenvalue bounds <span class="math notranslate nohighlight">\([a,b]\)</span> are valid.</p>
<p>This method utilizes a two-step recurrence relation:</p>
<div class="math notranslate nohighlight">
\[x^{(k+1)} = \alpha_k x^{(k)} + (1 - \alpha_k) x^{(k-1)} - \beta_k r^{(k)}, \quad k= 1, 2, \dots\]</div>
<p>where the iteration step starts with a special first step:</p>
<div class="math notranslate nohighlight">
\[x^{(1)} = x^{(0)} - \frac{1}{2} \beta_0 r^{(0)}.\]</div>
<p><span class="math notranslate nohighlight">\(r^{(k)}\)</span> is the residual:</p>
<div class="math notranslate nohighlight">
\[r^{(k)} = Ax^{(k)} - b\]</div>
<p>The construction relates the error polynomials <span class="math notranslate nohighlight">\(P_k(A)\)</span> generated by this recurrence to the standard three-term recurrence relation of the Chebyshev polynomials of the first kind:</p>
<div class="math notranslate nohighlight">
\[T_{k+1}(z) = 2zT_k(z) - T_{k-1}(z), \quad k = 1, 2, \dots\]</div>
<p>To ensure the recursion for the error vector <span class="math notranslate nohighlight">\(e^{(k)}\)</span> follows the structure derived from the normalized Chebyshev polynomial recurrence, the parameters <span class="math notranslate nohighlight">\(\alpha_k\)</span> and <span class="math notranslate nohighlight">\(\beta_k\)</span> must be chosen such that the error polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span> matches <span class="math notranslate nohighlight">\(T_k(z(\lambda))/T_k(z(0))\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> be the lower and upper bounds, respectively, of the eigenvalues <span class="math notranslate nohighlight">\(\lambda_j\)</span> of <span class="math notranslate nohighlight">\(A\)</span>, such that <span class="math notranslate nohighlight">\(0 &lt; a &lt; \lambda_j &lt; b\)</span>. The optimal parameters are defined as, <span class="math notranslate nohighlight">\(k \ge 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\alpha_k = \frac{a+b}{2} \beta_k, \\[1em]
\beta_k^{-1} = \frac{a+b}{2} - \left(\frac{b-a}{4}\right)^2 \beta_{k-1},
\end{split}\]</div>
<p>with starting parameter</p>
<div class="math notranslate nohighlight">
\[\beta_0 = \frac{4}{a+b}\]</div>
<p>The primary advantage of this second-order method is that <strong>it gives the optimal result for every step <span class="math notranslate nohighlight">\(k\)</span></strong> (apart from rounding errors), eliminating the need to choose the total number of iterations <span class="math notranslate nohighlight">\(k\)</span> a priori.</p>
</section>
<section id="advantages-of-the-2nd-order-chebyshev-iterative-method">
<h3>Advantages of the 2nd Order Chebyshev Iterative Method<a class="headerlink" href="#advantages-of-the-2nd-order-chebyshev-iterative-method" title="Link to this heading">#</a></h3>
<p>The second-order method offers distinct advantages over its first-order counterpart:</p>
<ul class="simple">
<li><p><strong>Numerical Stability:</strong> The two-step version of the Chebyshev iterative method is <strong>numerically stable</strong>.</p></li>
<li><p><strong>Optimal Performance at Every Step:</strong> Unlike the one-step method which optimizes the result only for a predetermined step <span class="math notranslate nohighlight">\(k\)</span>, the two-step method is designed so that, apart from rounding errors, it gives the <strong>optimal result for every step <span class="math notranslate nohighlight">\(k\)</span></strong>.</p></li>
<li><p><strong>Elimination of A Priori Step Count:</strong> The method eliminates the requirement to choose the number of iteration steps <span class="math notranslate nohighlight">\(k\)</span> <em>a priori</em>.</p></li>
</ul>
<p>Both the first-order (when stable) and the second-order methods share the optimal asymptotic rate of convergence, which increases only as the square root of the condition number of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</section>
</section>
<section id="requirements-and-sensitivity-to-eigenvalue-bounds">
<h2>Requirements and Sensitivity to Eigenvalue Bounds<a class="headerlink" href="#requirements-and-sensitivity-to-eigenvalue-bounds" title="Link to this heading">#</a></h2>
<p>The Chebyshev iteration method has <strong>excellent convergence properties</strong> because the number of iterations required for a constant error reduction is proportional to <span class="math notranslate nohighlight">\(\sqrt{\kappa}\)</span>, which is a vast improvement over basic stationary methods (like Jacobi or Gauss-Seidel) where the number of iterations is typically proportional to <span class="math notranslate nohighlight">\(\kappa\)</span>.</p>
<p>However, this method <strong>requires precise knowledge of the interval that contains the eigenvalues</strong> of the iteration matrix.</p>
<ol class="arabic simple">
<li><p><strong>Real and Positive/Negative Eigenvalues:</strong> The basic method is derived assuming the eigenvalues are real and positive (or all negative, allowing for a transformation). If the eigenvalues are complex, the method is still applicable, provided they are contained within an ellipse in the right or left half of the complex plane, symmetric with respect to the real axis.</p></li>
<li><p><strong>Sensitivity to the Lower Bound:</strong> For the optimal convergence rate derived above, the method critically depends on the ratio <span class="math notranslate nohighlight">\(b/a\)</span>, where <span class="math notranslate nohighlight">\(a\)</span> is the smallest positive eigenvalue.</p>
<ul class="simple">
<li><p>In many problems, particularly those arising from discretized PDEs, the <strong>smallest eigenvalues (<span class="math notranslate nohighlight">\(a\)</span>) can be very close to 0</strong> (e.g., <span class="math notranslate nohighlight">\(O(h^2)\)</span> in many finite difference schemes).</p></li>
<li><p>Determining this lower bound <span class="math notranslate nohighlight">\(a\)</span> accurately can be <strong>difficult and computationally expensive</strong>.</p></li>
<li><p>If the estimate for the lower bound <span class="math notranslate nohighlight">\(a\)</span> is slightly too small, the estimated condition number <span class="math notranslate nohighlight">\(\kappa = b/a\)</span> becomes inflated, leading to a much slower predicted rate of convergence. If the estimate for <span class="math notranslate nohighlight">\(a\)</span> is too large (i.e., outside the true interval), the method may diverge.</p></li>
<li><p>The convergence rate curve can be <strong>extremely sensitive near the optimal value</strong> of the parameters, highlighting the danger of slightly inaccurate eigenvalue estimates.</p></li>
</ul>
</li>
</ol>
</section>
<section id="pros-cons-and-applications">
<h2>Pros, Cons, and Applications<a class="headerlink" href="#pros-cons-and-applications" title="Link to this heading">#</a></h2>
<p>The method’s reliance on precise spectral bounds means that despite its theoretical efficiency, it is often avoided in favor of <strong>parameter-free methods</strong> like the Conjugate Gradient (CG) method, which typically converges even faster in a certain norm without requiring eigenvalue estimates.</p>
<p>The following table summarizes the key properties, advantages (pros), and disadvantages (cons) of the 1st order (one-step) and 2nd order (two-step) Chebyshev iterative methods:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Feature</p></th>
<th class="head text-left"><p>1st Order Chebyshev Iterative Method (One-Step)</p></th>
<th class="head text-left"><p>2nd Order Chebyshev Iterative Method (Two-Step)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Recurrence Relation</strong></p></td>
<td class="text-left"><p>Uses a one-step recurrence relation: <span class="math notranslate nohighlight">\(x^{k+1} = x^k - \tau_k(Ax^k - b)\)</span>.</p></td>
<td class="text-left"><p>Uses a two-step recurrence relation: <span class="math notranslate nohighlight">\(x^{k+1} = \alpha_k x^k + (1 - \alpha_k) x^{k-1} - \beta_k r^k\)</span>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Primary Pros</strong></p></td>
<td class="text-left"><p>Parameters are computed from a <strong>closed expression</strong>—that is, the one-step method’s parameters are explicit functions of <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, and <span class="math notranslate nohighlight">\(k\)</span>. Shares the <strong>optimal asymptotic rate of convergence</strong> (proportional to the square root of the condition number <span class="math notranslate nohighlight">\(O(\sqrt{\kappa})\)</span>).</p></td>
<td class="text-left"><p><strong>Numerically stable</strong>. Gives the <strong>optimal result for every step <span class="math notranslate nohighlight">\(k\)</span></strong>, apart from rounding errors, unlike the one-step method which is only optimized for a pre-chosen final step <span class="math notranslate nohighlight">\(k\)</span>. Eliminates the requirement to choose the number of iteration steps <span class="math notranslate nohighlight">\(k\)</span> <em>a priori</em>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Primary Cons</strong></p></td>
<td class="text-left"><p>Major disadvantage is potential <strong>numerical instability</strong>. Instability occurs because, for certain iteration indices <span class="math notranslate nohighlight">\(k\)</span>, the iteration matrices can have a spectral radius much larger than 1. Requires the user to choose the total number of iteration steps <span class="math notranslate nohighlight">\(k\)</span> <strong>a priori</strong>. The iteration parameters <span class="math notranslate nohighlight">\(\tau_k\)</span> must be selected and used in a specific order (large values balanced by small values) to mitigate instability.</p></td>
<td class="text-left"><p>Parameters are derived from <strong>recursions</strong>. Requires more memory and complexity due to the two-step dependence.</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Shared Requirements</strong>: Requires estimates of extreme eigenvalues for optimal performance. Convergence rate curve can be <strong>extremely sensitive near the optimal value</strong> of the parameters if eigenvalue estimates are inaccurate.</p>
<p><strong>Applications and Use Cases:</strong></p>
<ol class="arabic simple">
<li><p><strong>Preconditioning:</strong> The method is applicable when combined with a preconditioning matrix <span class="math notranslate nohighlight">\(M\)</span> (such as SSOR) to solve the system <span class="math notranslate nohighlight">\(M^{-1}A\mathbf{x} = M^{-1}\mathbf{b}\)</span>, provided <span class="math notranslate nohighlight">\(M^{-1}A\)</span> has positive eigenvalues. This combination (SSOR with Chebyshev acceleration) can substantially reduce the iteration count.</p></li>
<li><p><strong>Other Specific Matrix Classes:</strong> Chebyshev methods can be extended to systems where the eigenvalues lie in two disjoint intervals or ellipses (such as indefinite symmetric matrices after transformation).</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="sor_method.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Successive Over-Relaxation (SOR) Method</p>
      </div>
    </a>
    <a class="right-next"
       href="krylov_iterative_methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Krylov Subspace Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-stationary-to-nonstationary-iteration">From Stationary to Nonstationary Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-and-matrix-polynomials">Error Propagation and Matrix Polynomials</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-polynomial-optimization-problem">The Polynomial Optimization Problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-representation-and-the-minimax-problem">Error Representation and the Minimax Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-optimal-polynomial-p-k-lambda">Deriving the Optimal Polynomial <span class="math notranslate nohighlight">\(P_k(\lambda)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-exact-bound">Calculating the Exact Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asymptotic-rate">Asymptotic Rate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-the-1st-order-chebyshev-iterative-method">Limitations of the 1st Order Chebyshev Iterative Method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-build-the-2nd-order-chebyshev-iterative-method">How to Build the 2nd Order Chebyshev Iterative Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-the-2nd-order-chebyshev-iterative-method">Advantages of the 2nd Order Chebyshev Iterative Method</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements-and-sensitivity-to-eigenvalue-bounds">Requirements and Sensitivity to Eigenvalue Bounds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-cons-and-applications">Pros, Cons, and Applications</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Eric Darve
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>